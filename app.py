import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from io import BytesIO

# ‡πÇ‡∏´‡∏•‡∏î config ‡∏à‡∏≤‡∏Å toml
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
GOOGLE_CSE_ID = st.secrets["GOOGLE_CSE_ID"]

# ---------------------------
# Sidebar: Input
# ---------------------------
st.sidebar.title("üîç Search Settings")

input_line = st.sidebar.text_input(
    "üìå Enter your search keywords",
    placeholder="e.g., ‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤, ‡∏Ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏¢, ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢"
)

site_option = st.sidebar.selectbox(
    "üåê Restrict search to specific site:",
    ["All sites", "facebook.com", "instagram.com", "x.com"]
)

max_results = st.sidebar.slider(
    "üî¢ Number of results", min_value=10, max_value=100, step=10, value=30
)

site_prefix = ""
if site_option != "All sites":
    site_prefix = f"site:{site_option}"

st.title("üìÑ Scraping Tool (with Google Custom Search API)")

st.markdown("""
‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏ô Google ‡∏ú‡πà‡∏≤‡∏ô Google Custom Search API ‡πÅ‡∏ö‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÄ‡∏ä‡πà‡∏ô Facebook ‡∏´‡∏£‡∏∑‡∏≠ Instagram  
‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏°‡∏°‡πà‡∏≤ (`,`) ‡∏Ñ‡∏±‡πà‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥  
""")
# ...
# Initialize session state
if input_line:
    # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏Å‡πà‡∏≤ ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πá‡∏Ñ last_query)
    st.session_state.results = []

    keywords = [k.strip() for k in input_line.split(",") if k.strip()]
    query = f"{site_prefix} {' '.join(keywords)}".strip()

    st.markdown(f"### üîé Results for query: `{query}`")

    results_per_page = 10
    total_pages = max_results // results_per_page
    urls = []
    total_results_reported = None

    try:
        with st.spinner("üîÑ Searching Google via Custom Search API..."):
            for page in range(total_pages):
                start_index = page * results_per_page + 1
                params = {
                    "key": GOOGLE_API_KEY,
                    "cx": GOOGLE_CSE_ID,
                    "q": query,
                    "num": results_per_page,
                    "start": start_index,
                    "hl": "th"
                }
                resp = requests.get("https://www.googleapis.com/customsearch/v1", params=params)
                data = resp.json()

                if total_results_reported is None:
                    total_results_reported = data.get("searchInformation", {}).get("totalResults", "0")

                items = data.get("items", [])
                urls.extend([item["link"] for item in items])
                if len(items) < results_per_page:
                    break
    except Exception as e:
        st.error(f"‚ùå Error during search: {e}")
        urls = []

    results = []
    for i, url in enumerate(urls, start=1):
        try:
            response = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(response.text, 'html.parser')

            title = soup.title.string.strip() if soup.title else "No title found"

            meta_desc = soup.find("meta", attrs={"name": "description"}) or \
                        soup.find("meta", attrs={"property": "og:description"})
            if meta_desc and meta_desc.get("content"):
                description = meta_desc["content"].strip()
            else:
                first_p = soup.find("p")
                description = first_p.text.strip()[:300] if first_p else "No content found"

        except Exception as e:
            title = f"Error fetching title: {e}"
            description = f"Error fetching content: {e}"

        with st.expander(f"{i}. {title}"):
            st.write(f"üîó {url}")
            st.write(f"‚úèÔ∏è {description}")

        results.append({
            "No.": i,
            "Title": title,
            "URL": url,
            "Content": description
        })

    st.info(f"üìä ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏à‡∏≤‡∏Å Google ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_results_reported} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")

    st.session_state.results = results
    st.session_state.last_query = input_line + site_option + str(max_results)


# ---------------------------
# Display & Export
# ---------------------------
if st.session_state.results:
    for r in st.session_state.results:
        with st.expander(f"{r['No.']}. {r['Title']}"):
            st.write(f"üîó [Visit Site]({r['URL']})")
            st.write(f"‚úèÔ∏è {r['Content']}")

    df = pd.DataFrame(st.session_state.results)

    file_format = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î", ["CSV", "Excel"])
    data = None
    file_name = ""
    mime = ""

    if file_format == "CSV":
        data = df.to_csv(index=False).encode("utf-8")
        file_name = "search_results.csv"
        mime = "text/csv"
    elif file_format == "Excel":
        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="Results")
        data = output.getvalue()
        file_name = "search_results.xlsx"
        mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

    if data:
        st.download_button(
            label=f"üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {file_name}",
            data=data,
            file_name=file_name,
            mime=mime
        )
